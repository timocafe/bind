/*
 * Copyright Institute for Theoretical Physics, ETH Zurich 2015.
 * Distributed under the Boost Software License, Version 1.0.
 *
 * Permission is hereby granted, free of charge, to any person or organization
 * obtaining a copy of the software and accompanying documentation covered by
 * this license (the "Software") to use, reproduce, display, distribute,
 * execute, and transmit the Software, and to prepare derivative works of the
 * Software, and to permit third-parties to whom the Software is furnished to
 * do so, all subject to the following:
 *
 * The copyright notices in the Software and this entire statement, including
 * the above license grant, this restriction and the following disclaimer,
 * must be included in all copies of the Software, in whole or in part, and
 * all derivative works of the Software, unless such copies or derivative
 * works are solely in the form of machine-executable object code generated by
 * a source language processor.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
 * SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
 * FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */

#ifndef BIND
#define BIND
#ifndef NDEBUG
#define NDEBUG
#define BIND_NO_DEBUG
#endif
// {{{ required packages
#if defined(BIND_REQUIRE_MPI) && !defined(MPI_VERSION)
#pragma message("Warning: mpi.h is required but wasn't included.")
#include <mpi.h>
#endif
#if defined(BIND_REQUIRE_CUDA) && !defined(CUDART_VERSION)
#pragma message("Warning: cuda_runtime.h is required but wasn't included.")
#include <cuda_runtime.h>
#endif
// }}}
// {{{ system includes
#include <complex>
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>
#include <string>
#include <cstring>
#include <limits>
#include <vector>
#include <stack>
#include <set>
#include <map>
#include <list>
#include <memory.h>
#include <stdarg.h>
#include <ctype.h>
#include <iostream>
#include <fstream>
#include <sys/time.h>
#include <algorithm>
#include <execinfo.h>
#include <stdexcept>
#include <type_traits>
#include <functional>
#include <utility>
#include <atomic>
#include <tuple>
#include <sys/mman.h>
#include <chrono>
// }}}
// {{{ utils package
#ifndef BIND_UTILS_INDEX_SEQUENCE
#define BIND_UTILS_INDEX_SEQUENCE

namespace bind {

    template<size_t... Indices>
    struct index_sequence {
        template<size_t N>
        using append = index_sequence<Indices..., N>;
    };

    template<size_t Size>
    struct make_index_sequence_impl {
        typedef typename make_index_sequence_impl<Size-1>::type::template append<Size-1> type;
    };

    template<>
    struct make_index_sequence_impl<0u> {
        typedef index_sequence<> type;
    };

    template<size_t Size>
    using make_index_sequence = typename make_index_sequence_impl<Size>::type;

}

#endif

#ifndef BIND_UTILS_IO
#define BIND_UTILS_IO

namespace bind { namespace utils {

    class funneled_io {
    public:
        funneled_io() : nullio("/dev/null"), latch(NULL) { }
       ~funneled_io(){
            disable();
        }
        void enable(){
            latch = std::cout.rdbuf();
            std::cout.rdbuf(nullio.rdbuf());
        }
        void disable(){
            if(!latch) return;
            std::cout.rdbuf(latch);
            latch = NULL;
        }
    private:
        std::ofstream nullio;
        std::streambuf* latch;
    };

} }

#endif

#ifndef BIND_UTILS_GUARD
#define BIND_UTILS_GUARD

namespace bind { 

    class guard_once {
    public:
        guard_once() : once(false) { }
        bool operator()(){ if(!once){ once = true; return true; } return false; }
    private:
        bool once;
    };
}

#endif

#ifndef BIND_UTILS_RANK_T
#define BIND_UTILS_RANK_T

namespace bind {
    #if 0
    class rank_t {
    public:
        rank_t(){}
        rank_t(int r) : rank(r) {}
        bool operator != (const rank_t& other) const { return (rank != other.rank); }
        bool operator == (const rank_t& other) const { return (rank == other.rank); }
        bool operator <  (const rank_t& other) const { return (rank < other.rank);  }
        bool operator >  (const rank_t& other) const { return (rank > other.rank);  }
        int& toint() const { return rank; }
        mutable int rank; // mutable due to MPI
    };
    #else
    typedef int rank_t;
    #endif
}

#endif

#ifndef BIND_UTILS_THREADING
#define BIND_UTILS_THREADING

#define CILK    1
#define OPENMP  2

// default: select threading if icc/gcc
#if !defined(BIND_THREADING)
#if defined __INTEL_COMPILER
#define BIND_THREADING CILK
#elif defined __GNUC__
#define BIND_THREADING OPENMP
#endif
#endif

#if BIND_THREADING == CILK
  #include <cilk/cilk.h>
  #include <cilk/cilk_api.h>
  #define BIND_CILK
  #define BIND_THREADING_TAGLINE "using cilk"
  #define BIND_NUM_THREADS __cilkrts_get_nworkers()
  #define BIND_THREAD_ID __cilkrts_get_worker_number()
  #define BIND_THREAD cilk_spawn
  #define BIND_SMP_ENABLE
  #define BIND_SMP_DISABLE cilk_sync;
  #define BIND_PARALLEL_FOR(...) cilk_for(__VA_ARGS__)
#elif BIND_THREADING == OPENMP
  #include <omp.h>
  #define BIND_OMP
  #define BIND_THREADING_TAGLINE "using openmp"
  #define BIND_THREAD_ID omp_get_thread_num()
  #define BIND_PRAGMA(a) _Pragma( #a )
  #define BIND_THREAD BIND_PRAGMA(omp task untied)
  #define BIND_PARALLEL_FOR(...) BIND_PRAGMA(omp parallel for schedule(dynamic, 1)) for(__VA_ARGS__)
  #define BIND_SMP_ENABLE BIND_PRAGMA(omp parallel) { BIND_PRAGMA(omp single nowait)
  #define BIND_SMP_DISABLE }
  #define BIND_NUM_THREADS [&]()->int{ int n; BIND_SMP_ENABLE \
                              { n = omp_get_num_threads(); } \
                              BIND_SMP_DISABLE return n; }()
#else
  #define BIND_PARALLEL_FOR(...) for(__VA_ARGS__)
  #define BIND_THREADING_TAGLINE "no threading"
  #define BIND_NUM_THREADS 1
  #define BIND_THREAD_ID   0
  #define BIND_THREAD
  #define BIND_SMP_ENABLE
  #define BIND_SMP_DISABLE
#endif

#endif
// }}}
// {{{ memory package

#ifndef BIND_MEMORY_TYPES
#define BIND_MEMORY_TYPES

namespace bind { namespace memory {
    // {{{ types lookup detail
    namespace detail {
        template<int N, int Limit>
        constexpr int lower_bound(){
            return (N > Limit ? N : Limit);
        }

        template<int Offset>
        struct checked_get { static constexpr int value = Offset; };

        template<>
        struct checked_get< -1 > { /* type not found */ };

        template<class T, class Tuple, int I = std::tuple_size<Tuple>::value>
        constexpr int find_type(){
            return I ? std::is_same< typename std::tuple_element<lower_bound<I-1,0>(),Tuple>::type, T >::value ? I-1 : find_type<T,Tuple,lower_bound<I-1,0>()>()
                     : -1;
        }
    }
    // }}}

    namespace cpu {
        class standard;
        class bulk;
    }
    namespace gpu {
        class pinned;
        class standard;
    }

    struct types {
        typedef int id_type;
        typedef std::tuple< memory::cpu::bulk,
                            memory::cpu::standard,
                            memory::gpu::pinned,
                            memory::gpu::standard
                            > list;

        template<typename T>
        using id = detail::checked_get< detail::find_type<T,list>() >;

        struct cpu {
            static constexpr id_type bulk = id<memory::cpu::bulk>::value;
            static constexpr id_type standard = id<memory::cpu::standard>::value;
        };
        struct gpu {
            static constexpr id_type pinned = id<memory::gpu::pinned>::value;
            static constexpr id_type standard = id<memory::gpu::standard>::value;
        };
        static constexpr id_type none = std::tuple_size<list>::value;
    };
} }

#endif

#ifndef BIND_MEMORY_FACTORY
#define BIND_MEMORY_FACTORY

namespace bind { namespace memory {

    template<size_t S>
    class private_factory {
    public:
        private_factory(){
            this->buffers.push_back(std::malloc(S));
            this->buffer = &this->buffers[0];
        }
       ~private_factory(){
            for(int i = 0; i < buffers.size(); i++) 
                std::free(this->buffers[i]);
        }
        void* provide(){
            void* chunk = *buffer;
            if(chunk == buffers.back()){
                buffers.push_back(std::malloc(S));
                buffer = &buffers.back();
            }else
                buffer++;
            return chunk;
        }
        void reset(){
            buffer = &buffers[0];
        }
        size_t size(){
            return (buffer - &buffers[0]);
        }
    private:
        std::vector<void*> buffers;
        void** buffer;
    };

} }

#endif

#ifndef BIND_MEMORY_REGION
#define BIND_MEMORY_REGION

namespace bind { namespace memory {

    constexpr size_t aligned_64(size_t size){ return 64 * (size_t)((size+63)/64); }
    template<size_t S> constexpr size_t aligned_64(){ return 64 * (size_t)((S+63)/64); }

    template<size_t S, class Factory>
    class serial_region {
    public:
        serial_region(){
            this->buffer = NULL;
            this->iterator = (char*)this->buffer+S;
        }
        void realloc(){
            this->buffer = Factory::provide();
            this->iterator = (char*)this->buffer;
        }
        void* malloc(size_t sz){
            if(((size_t)iterator + sz - (size_t)this->buffer) >= S) realloc();
            void* m = (void*)iterator;
            iterator += aligned_64(sz);
            return m;
        }
        void reset(){
            this->iterator = (char*)this->buffer+S;
        }
    protected:
        void* buffer;
        char* iterator;
    };

    template<size_t S, class Factory>
    class private_region : public serial_region<S,Factory> {
    public:
        typedef serial_region<S,Factory> base;
        void* malloc(size_t sz){
            if(((size_t)this->iterator + sz - (size_t)this->buffer) >= S){
                this->buffer = pool.provide();
                this->iterator = (char*)this->buffer;
            }
            void* m = (void*)this->iterator;
            this->iterator += aligned_64(sz);
            return m;
        }
        void reset(){
            base::reset();
            pool.reset();
        }
    private:
        Factory pool;
    };

} }

#endif
    // {{{ memory::cpu package

#ifndef BIND_MEMORY_CPU_BULK
#define BIND_MEMORY_CPU_BULK

#ifndef BIND_INSTR_BULK_CHUNK
#define BIND_INSTR_BULK_CHUNK 16777216 // 16 MB
#endif

namespace bind { namespace memory { namespace cpu {

    struct bulk {
        static constexpr int type = types::cpu::bulk;
        bulk(const bulk&) = delete;
        bulk& operator=(const bulk&) = delete;
    protected:
        bulk() = default;
    };

} } }

#endif

#ifndef BIND_MEMORY_CPU_INSTR_BULK
#define BIND_MEMORY_CPU_INSTR_BULK

namespace bind { namespace memory { namespace cpu {

    struct instr_bulk {
        template<class T>
        class allocator {
        public:
            typedef T value_type;
            template <class U> struct rebind { typedef allocator<U> other; };
            allocator() throw() { }
            allocator(const allocator&) throw() { }
            template<typename U> allocator(const allocator<U>&) throw() { }
           ~allocator() throw() { }
            static void deallocate(T* p, size_t n){ }
            static T* allocate(size_t n){
                return (T*)instr_bulk::malloc(n*sizeof(T));
            }
        };

        static instr_bulk& instance(){
            static instr_bulk singleton; return singleton;
        }
        template<size_t S> 
        static void* malloc(){
            return malloc(S);
        }
        static void* malloc(size_t s){
            return instance().impl.malloc(s);
        }
        static void drop(){
            instance().impl.reset();
        }
    private:
        memory::private_region<BIND_INSTR_BULK_CHUNK, 
                               memory::private_factory<BIND_INSTR_BULK_CHUNK> 
                              > impl;
    };

} } }

#endif


#ifndef BIND_MEMORY_CPU_STANDARD
#define BIND_MEMORY_CPU_STANDARD

namespace bind { namespace memory { namespace cpu {

    struct standard {
        static constexpr int type = types::cpu::standard;

        template<size_t S> static void* malloc(){ return std::malloc(S);   }
        template<size_t S> static void* calloc(){ return std::calloc(1,S); }

        static void* malloc(size_t sz){ return std::malloc(sz); }
        static void* calloc(size_t sz){ return std::calloc(1,sz); }

        static void free(void* ptr){ std::free(ptr);  }
    };

} } }

#endif

#ifndef BIND_MEMORY_CPU_NEW
#define BIND_MEMORY_CPU_NEW

namespace bind { namespace memory { namespace cpu {

    template<class T>
    class use_fixed_new {
    public:
        void* operator new (size_t sz){ assert(sz == sizeof(T)); return memory::cpu::standard::malloc<sizeof(T)>(); }
        void operator delete (void* ptr){ memory::cpu::standard::free(ptr); }
    };

    template<class T>
    class use_bulk_new {
    public:
        void* operator new (size_t sz){ assert(sz == sizeof(T)); return memory::cpu::instr_bulk::malloc<sizeof(T)>(); }
        void operator delete (void* ptr){ }
    };

} } }

#endif
    // }}}
    // {{{ memory::gpu package
    #ifdef CUDART_VERSION

#ifndef BIND_MEMORY_GPU_PINNED
#define BIND_MEMORY_GPU_PINNED

namespace bind { namespace memory { namespace gpu {

    struct pinned {
	static constexpr int type = types::gpu::pinned;

	template<size_t S> static void* malloc(){ void* ptr; cudaMallocHost(&ptr, S); return ptr; }
	template<size_t S> static void* calloc(){ void* ptr = malloc<S>(); memset(ptr, 0, S); return ptr; }

	static void* malloc(size_t sz){ void* ptr; cudaMallocHost(&ptr, sz); return ptr; }
	static void* calloc(size_t sz){ void* ptr = malloc(sz); memset(ptr, 0, sz); return ptr; }

	static void free(void* ptr){ cudaFreeHost(ptr); }
    };

} } }

#endif

#ifndef BIND_MEMORY_GPU_STANDARD
#define BIND_MEMORY_GPU_STANDARD

namespace bind { namespace memory { namespace gpu {

    struct standard {
        static constexpr int type = types::gpu::standard;

        template<size_t S> static void* malloc(){ void* ptr; cudaMalloc(&ptr, S); return ptr; }
        template<size_t S> static void* calloc(){ void* ptr = malloc<S>(); cudaMemset(ptr, 0, S); return ptr; }

        static void* malloc(size_t sz){ void* ptr; cudaMalloc(&ptr, sz); return ptr; }
        static void* calloc(size_t sz){ void* ptr = malloc(sz); cudaMemset(ptr, 0, sz); return ptr; }

        static void free(void* ptr){ cudaFree(ptr); }
    };

} } }

#endif
    #endif
    // }}}

#ifndef BIND_MEMORY_DESCRIPTOR
#define BIND_MEMORY_DESCRIPTOR

namespace bind { namespace memory {

    template<class MemoryTypes>
    struct hub;

    struct descriptor {
        template<class MemoryTypes> friend struct hub;
        descriptor(size_t e, types::id_type t = types::none) : extent(e), type(t) {}

        void free(void* ptr){
            if(!ptr) return;
            switch(type){
                case types::none: return;
                case types::cpu::standard: cpu::standard::free(ptr); break;
                #ifdef CUDART_VERSION
                case types::gpu::standard: gpu::standard::free(ptr); break;
                #endif
                default: return;
            }
            type = types::none;
        }
        template<class Memory>
        void* hard_malloc(){
            type = Memory::type;
            return Memory::malloc(extent);
        }
        template<class MemoryTypes>
        void* malloc(){
            return hub<MemoryTypes>::malloc(*this);
        }
        template<class MemoryTypes>
        void* calloc(){
            void* m = hub<MemoryTypes>::malloc(*this);
            hub<MemoryTypes>::memset(*this, m);
            return m;
        }
        template<class MemoryTypes>
        void memcpy(void* dst, void* src, descriptor& src_desc){
            hub<MemoryTypes>::memcpy(*this, dst, src_desc, src);
        }
        template<class MemoryTypes>
        bool conserves(descriptor& p){
            return hub<MemoryTypes>::conserves(*this, p);
        }
        void reuse(descriptor& d){
            type = d.type;
            d.type = types::none;
        }
    public:
        const size_t extent;
    private:
        types::id_type type;
    };

} }

#endif

#ifndef BIND_MEMORY_HUB
#define BIND_MEMORY_HUB

namespace bind { namespace memory {

    template<class MemoryTypes>
    struct hub {
        static bool conserves(descriptor& c, descriptor& p){
            return true;
        }
        static void* malloc(descriptor& c){
            c.type = types::cpu::standard;
            return cpu::standard::malloc(c.extent);
        }
        static void memset(descriptor& desc, void* ptr){
            std::memset(ptr, 0, desc.extent);
        }
        static void memcpy(descriptor& dst_desc, void* dst, descriptor& src_desc, void* src){
            std::memcpy(dst, src, src_desc.extent);
        }
    };

    #ifdef CUDART_VERSION
    template<>
    struct hub<types::cpu> : public hub<void> {
        static bool is_sibling(descriptor& c){
            return c.type != types::gpu::standard;
        }
        static bool conserves(descriptor& c, descriptor& p){
            return is_sibling(p);
        }
    };

    template<>
    struct hub<types::gpu> {
        static bool is_sibling(descriptor& c){
            return c.type == types::gpu::standard;
        }
        static bool conserves(descriptor& c, descriptor& p){
            return is_sibling(p);
        }
        static void* malloc(descriptor& c){
            c.type = types::gpu::standard;
            return gpu::standard::malloc(c.extent);
        }
        static void memset(descriptor& desc, void* ptr){
            cudaMemset(ptr, 0, desc.extent);
        }
        static void memcpy(descriptor& dst_desc, void* dst, descriptor& src_desc, void* src){
            cudaMemcpy(dst, src, src_desc.extent, cudaMemcpyDeviceToDevice);
        }
    };
    #endif
} }

#endif
// }}}
// {{{ model package

#ifndef BIND_MODEL_DEVICE
#define BIND_MODEL_DEVICE

namespace bind {
    enum class device { cpu, gpu, any };

    template<device D>
    struct associated_memory_types {
        using set = memory::types::cpu;
    };

    template<>
    struct associated_memory_types<device::gpu> {
        using set = memory::types::gpu;
    };
}

#endif

#ifndef BIND_MODEL_LOCALITY
#define BIND_MODEL_LOCALITY

namespace bind {
    enum class locality { remote, local, common };
}

#endif

#ifndef BIND_MODEL_FUNCTOR
#define BIND_MODEL_FUNCTOR

namespace bind { namespace model {
    
    class functor {
        typedef memory::cpu::instr_bulk::allocator<functor*> allocator_type;
    public:
        virtual void invoke() = 0;
        virtual bool ready() = 0;
        void queue(functor* d){ deps.push_back(d); }
        std::vector<functor*, allocator_type> deps;
        void* arguments[1]; // note: trashing the vtptr of derived object
    };

} }

#endif

#ifndef BIND_MODEL_REVISION
#define BIND_MODEL_REVISION

namespace bind { namespace model {

    class revision : public memory::cpu::use_fixed_new<revision> {
    public:
        revision(size_t extent, functor* g, locality l, device d, rank_t owner)
        : spec(extent), generator(g), state(l), dev(d), owner(owner),
          data(NULL), users(0), crefs(1)
        {
        }
        template<device D>
        revision* clone(){
            return new revision(spec.extent, NULL, state, D, owner);
        }
        void embed(void* ptr){
            data = ptr;
        }
        void reuse(revision& r){
            data = r.data;
            spec.reuse(r.spec);
        }
        void use(){
            ++users;
        }
        void release(){
            --users;
        }
        void complete(){
            generator = NULL;
        }
        void invalidate(){
            data = NULL;
        }
        bool locked() const {
            return (users != 0);
        }
        bool locked_once() const {
            return (users == 1);
        }
        bool valid() const {
            return (data != NULL);
        }
        bool referenced() const {
            return (crefs != 0);
        }
        void protect(){
            crefs++;
        }
        void weaken(){
            crefs--;
        }

        memory::descriptor spec;
        std::atomic<functor*> generator;
        const locality state;
        const device dev;
        rank_t owner;
        void* data;
        std::atomic<int> users;
        int crefs;
        std::pair<size_t, functor*> assist;
    };

    inline bool cpu(const revision* r){
        return (r->dev == device::cpu);
    }

    inline bool gpu(const revision* r){
        return (r->dev == device::gpu);
    }

    inline bool local(const revision* r){
        return (r->state == locality::local);
    }
    
    inline bool remote(const revision* r){
        return (r->state == locality::remote);
    }
    
    inline bool common(const revision* r){
        return (r->state == locality::common);
    }
    
    inline rank_t owner(const revision* r){
        return r->owner;
    }

} }

#endif

#ifndef BIND_MODEL_HISTORY
#define BIND_MODEL_HISTORY

// revision tracking mechanism (target selector)
namespace bind { namespace model {

    class history : public memory::cpu::use_fixed_new<history> {
    public:
        history(size_t size) : current(NULL), shadow(NULL), extent(memory::aligned_64(size)) { }
        template<device D>
        void init_state(rank_t owner){
            revision* r = new revision(extent, NULL, locality::common, D, owner);
            this->current = r;
        }
        template<locality L, device D>
        void add_state(functor* gen, rank_t owner){
            revision* r = new revision(extent, gen, L, D, owner);
            this->current = r;
        }
        revision* back() const {
            return this->current;
        }
        bool weak() const {
            return (this->back() == NULL);
        }
        revision* current;
        revision* shadow;
        size_t extent;
    };

} }

#endif

#ifndef BIND_MODEL_ANY
#define BIND_MODEL_ANY

namespace bind { namespace model {

    template<typename T>
    constexpr size_t sizeof_any(){
        return (2*sizeof(void*) + sizeof(size_t) + memory::aligned_64<sizeof(T)>());
    }

    class any {
    public:
        // WARNING: the correct allocation of sizeof_any required
        void* operator new (size_t, void* place){ return place; }
        void operator delete (void*, void*){}

        template<typename T>
        any(T val) : size(memory::aligned_64<sizeof(T)>()) { *this = val; }
        template<typename T> void operator = (T val){ *(T*)&value = val; }
        template<typename T> operator T& (){ return *(T*)&value;  }
        void complete(){ generator = NULL; }

        std::atomic<functor*> generator;
        any* origin;
        size_t size;
        int value;
    };

} }

#endif
// }}}
// {{{ transport package (requires :model)
#ifdef MPI_VERSION
#define BIND_CHANNEL_NAME mpi

#ifndef BIND_TRANSPORT_MPI_GROUP
#define BIND_TRANSPORT_MPI_GROUP

namespace bind { namespace transport { namespace mpi {

    struct group {
        group(MPI_Comm parent) : mpi_comm(parent)
        {
            MPI_Comm_group(this->mpi_comm,  &this->mpi_group);
            MPI_Group_size(this->mpi_group, &this->size);
            MPI_Group_rank(this->mpi_group, &this->rank);
        }
        int rank;
        int size;
        MPI_Comm  mpi_comm;
        MPI_Group mpi_group;
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_TREE
#define BIND_TRANSPORT_MPI_TREE

#define BOUNDARY_OVERFLOW -1

namespace bind {

    template<typename T>
    class binary_tree {
    public:
        binary_tree(size_t N) : tree(N), length(N) {
            entry_point = N/2;
            generate(tree, N, N);
            normalize();
            check();
        }
        T generate(std::vector<std::pair<T,T> >& tree, int N, int L, int start = 0){
            if(N <= 0 || start >= L) return BOUNDARY_OVERFLOW;
            tree[start+N/2] = std::make_pair(generate(tree, N/2, L, start), 
                                             generate(tree, N-N/2-1, L, start+N/2+1));
            return start+N/2;
        }
        void normalize(){
            std::vector<std::pair<T,T> > normalized(length);
            for(int i = 0; i < length; i++){
                normalized[i] = tree[(i+entry_point)%length];
                if(normalized[i].first  != BOUNDARY_OVERFLOW) normalized[i].first  = (normalized[i].first  - entry_point + length) % length;
                if(normalized[i].second != BOUNDARY_OVERFLOW) normalized[i].second = (normalized[i].second - entry_point + length) % length;
            }
            std::swap(tree, normalized);
            entry_point = 0;
        }
        void check(){
            std::vector<bool> states(length);
            for(int i = 0; i < length; i++){
                if(tree[i].first  != BOUNDARY_OVERFLOW) states[tree[i].first]  = true;
                if(tree[i].second != BOUNDARY_OVERFLOW) states[tree[i].second] = true;
            }
            for(int i = 0; i < length; i++){
                if(!states[i] && i != entry_point) throw std::runtime_error("Error: no route to host");
            }
        }
        std::pair<T,T> operator[](int i) const {
            return tree[i];
        }
    private:
        std::vector<std::pair<T,T> > tree;
        int entry_point;
        size_t length;
    };
}

#undef BOUNDARY_OVERFLOW
#endif

#ifndef BIND_TRANSPORT_MPI_CHANNEL_H
#define BIND_TRANSPORT_MPI_CHANNEL_H

namespace bind { namespace transport { namespace mpi {

    class request_impl;
    template<class T> class collective;

    static void recv_impl(request_impl* r);
    static void send_impl(request_impl* r);
    static bool test_impl(request_impl* r);

    class channel {
    public:
        typedef typename model::revision block_type;
        typedef typename model::any scalar_type;
        template<class T> using collective_type = collective<T>;
        struct mount {
            mount(); 
           ~mount();
            std::vector<binary_tree<rank_t>*> trees;
            std::vector<rank_t> circle;
            int tag_ub;
            int sid;
            int self;
            int np;
        };
        static mount& setup(){ 
            static mount m; 
            return m; 
        }
       ~channel();
        channel();
        size_t dim() const;
        static void barrier();
        collective<block_type>* get(block_type& r);
        collective<block_type>* set(block_type& r);
        collective<scalar_type>* bcast(scalar_type& v, rank_t root);
        collective<scalar_type>* bcast(scalar_type& v);
        rank_t rank;
        group* world;
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_REQUEST_H
#define BIND_TRANSPORT_MPI_REQUEST_H

namespace bind { namespace transport { namespace mpi {

    class request_impl : public memory::cpu::use_bulk_new<request_impl> {
    public:
        request_impl(void(*impl)(request_impl*), typename channel::scalar_type& v, rank_t target, int tag = 0);
        request_impl(void(*impl)(request_impl*), typename channel::block_type& r, rank_t target, int tag = 0);
        inline bool operator()();
        void* data;
        int extent;
        int target; // MPI_INT
        MPI_Request mpi_request;
        void(*impl)(request_impl*);
        bool once;
        int tag;
    };

    class request {
        typedef memory::cpu::instr_bulk::allocator<request_impl*> allocator_type;
    public:
        bool operator()();
        void operator &= (request_impl* r);
        void operator += (request_impl* r);
    private:
        std::vector<request_impl*,allocator_type> primaries;
        std::vector<request_impl*,allocator_type> callbacks;
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_COLLECTIVE_H
#define BIND_TRANSPORT_MPI_COLLECTIVE_H

namespace bind { namespace transport { namespace mpi {

    template<typename T>
    class bcast {
        typedef memory::cpu::instr_bulk::allocator<int> allocator_type;
    public:
        void dispatch();
        bcast(T& o, rank_t root) : object(o), root(root), self(0) {}
    private:
        template<class C> friend class collective;
        T& object;
        std::vector<int,allocator_type> tags;
        rank_t root;
        int self;
        int size;
        rank_t* list;
        request impl; 
        guard_once once;
    };

    template<class T> class collective {};

    template<>
    class collective<typename channel::block_type> : public bcast<typename channel::block_type>, 
                                                     public memory::cpu::use_bulk_new<collective<typename channel::block_type> > {
        typedef memory::cpu::instr_bulk::allocator<int> allocator_type;
    public:
        collective(typename channel::block_type& r, rank_t root);
        void append(rank_t rank);
        bool involved();
        bool test();
    private:
        std::vector<bool,allocator_type> states;
        std::vector<rank_t,allocator_type> tree;
    };

    template<>
    class collective<typename channel::scalar_type> : public bcast<typename channel::scalar_type>, 
                                                      public memory::cpu::use_bulk_new<collective<typename channel::scalar_type> > {
    public:
        collective(typename channel::scalar_type& v, rank_t root);
        bool test();
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_REQUEST_HPP
#define BIND_TRANSPORT_MPI_REQUEST_HPP

namespace bind { namespace transport { namespace mpi {

    // type information required //
    inline request_impl::request_impl(void(*impl)(request_impl*), typename channel::scalar_type& v, rank_t target, int tag)
    : extent(v.size/sizeof(double)), 
      data(&v.value),
      target(target),
      impl(impl),
      tag(tag),
      once(false)
    {
    }
    // type information required //
    inline request_impl::request_impl(void(*impl)(request_impl*), typename channel::block_type& r, rank_t target, int tag)
    : extent(r.spec.extent/sizeof(double)), 
      data(r.data),
      target(target),
      impl(impl),
      tag(tag),
      once(false)
    {
    }
    inline bool request_impl::operator()(){
        if(!once){ impl(this); once = true; }
        return test_impl(this);
    }

    inline bool request::operator()(){
        int length = primaries.size();
        for(int i = 0; i < length; i++){
            if(!(*primaries[i])()) return false;
        }
        primaries.clear();
        length = callbacks.size();
        for(int i = 0; i < length; i++){
            if(!(*callbacks[i])()) return false;
        }
        return true;
    }
    inline void request::operator &= (request_impl* r){
        primaries.push_back(r);
    }
    inline void request::operator += (request_impl* r){
        callbacks.push_back(r);
    }

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_CHANNEL_HPP
#define BIND_TRANSPORT_MPI_CHANNEL_HPP

namespace bind { namespace transport { namespace mpi {

    inline void recv_impl(request_impl* r){
        MPI_Irecv(r->data, r->extent, MPI_DOUBLE, r->target, r->tag, MPI_COMM_WORLD, &r->mpi_request);
    }
    inline void send_impl(request_impl* r){
        MPI_Isend(r->data, r->extent, MPI_DOUBLE, r->target, r->tag, MPI_COMM_WORLD, &r->mpi_request);
    }
    inline bool test_impl(request_impl* r){
        int f = 0; MPI_Test(&r->mpi_request, &f, MPI_STATUS_IGNORE); return f;
    }

    inline channel::mount::mount(){
        int *ub, flag, level, zero = 0;
        MPI_Init_thread(&zero, NULL, MPI_THREAD_FUNNELED, &level); 
        if(level != MPI_THREAD_FUNNELED) throw std::runtime_error("Error: Wrong threading level");
        MPI_Comm_size(MPI_COMM_WORLD, &np);
        MPI_Comm_rank(MPI_COMM_WORLD, &self);
        MPI_Attr_get(MPI_COMM_WORLD, MPI_TAG_UB, &ub, &flag);
        this->tag_ub = flag ? *ub : 32767;
        this->sid = 1;
        
        trees.resize(2); // 0,1 are empty
        for(int i = 2; i <= np; i++)  trees.push_back(new binary_tree<rank_t>(i));
        for(int i = 0; i < 2*np; i++) circle.push_back(i % np);
    }

    inline channel::mount::~mount(){
        for(binary_tree<rank_t>* t : trees) delete t;
        MPI_Finalize();
    }
    
    inline channel::~channel(){
        delete this->world;
    }

    inline channel::channel(){
        channel::setup(); // making sure MPI is initialised
        this->world = new group(MPI_COMM_WORLD);
        this->rank = this->world->rank;
    }

    inline void channel::barrier(){
        MPI_Barrier(MPI_COMM_WORLD);
    }

    inline size_t channel::dim() const {
        return this->world->size;
    }

    inline collective<typename channel::scalar_type>* channel::bcast(scalar_type& v, rank_t root){
        return new collective<scalar_type>(v, root);
    }

    inline collective<typename channel::scalar_type>* channel::bcast(scalar_type& v){
        return new collective<scalar_type>(v, rank);
    }

    inline collective<typename channel::block_type>* channel::get(block_type& r){
        return new collective<block_type>(r, r.owner);
    }

    inline collective<typename channel::block_type>* channel::set(block_type& r){
        return new collective<block_type>(r, rank);
    }

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_COLLECTIVE_HPP
#define BIND_TRANSPORT_MPI_COLLECTIVE_HPP

#define BOUNDARY_OVERFLOW -1

namespace bind { namespace transport { namespace mpi {

    namespace detail {
        inline int self(){
            return channel::setup().self;
        }
        inline int num_procs(){
            return channel::setup().np;
        }
        inline int generate_sid(){
            return (++channel::setup().sid %= channel::setup().tag_ub);
        }
    }

    template<typename T>
    inline void bcast<T>::dispatch(){
        std::pair<rank_t,rank_t> lr = (*channel::setup().trees[size])[self];
        if(!self){ // self == root
            if(lr.first  != BOUNDARY_OVERFLOW) impl &= new request_impl(send_impl, object, list[lr.first],  tags[lr.first]);
            if(lr.second != BOUNDARY_OVERFLOW) impl &= new request_impl(send_impl, object, list[lr.second], tags[lr.second]);
        }else{
            impl &= new request_impl(recv_impl, object, MPI_ANY_SOURCE, tags[self]);
            if(lr.first  != BOUNDARY_OVERFLOW) impl += new request_impl(send_impl, object, list[lr.first],  tags[lr.first]);
            if(lr.second != BOUNDARY_OVERFLOW) impl += new request_impl(send_impl, object, list[lr.second], tags[lr.second]);
        }
    }

    inline collective<typename channel::block_type>::collective(typename channel::block_type& r, rank_t root) 
    : bcast<typename channel::block_type>(r, root), states(detail::num_procs()+1) {
        this->tree.push_back(root);
        this->tags.push_back(-1);
    }

    inline void collective<typename channel::block_type>::append(rank_t rank){
        if(!states[rank]){
            states[rank] = true;
            if(states.back()){
                for(int i = this->tags.size(); i <= detail::num_procs(); i++)
                    this->tags.push_back(detail::generate_sid());
                for(int i = 0; i < detail::num_procs(); i++)
                    this->states[i] = true;
            }else{
                if(rank == detail::self()) this->self = tree.size();
                this->tags.push_back(channel::setup().sid);
                this->tree.push_back(rank);
            }
        }
        detail::generate_sid();
    }

    inline bool collective<typename channel::block_type>::involved(){
        return states[detail::self()] || states.back();
    }

    inline bool collective<typename channel::block_type>::test(){
        if(this->once()){
            if(states.back()){
                this->size = detail::num_procs();
                this->list = &channel::setup().circle[root];
                this->self = (size + detail::self() - root) % size;
            }else{
                this->size = tree.size();
                this->list = &tree[0];
            }
            this->dispatch();
        }
        return this->impl();
    }

    inline collective<typename channel::scalar_type>::collective(typename channel::scalar_type& v, rank_t root)
    : bcast<typename channel::scalar_type>(v, root) {
        tags.reserve(detail::num_procs()+1);
        for(int i = 0; i <= detail::num_procs(); i++)
            this->tags.push_back(detail::generate_sid());
    }

    inline bool collective<typename channel::scalar_type>::test(){
        if(this->once()){
            this->size = detail::num_procs();
            this->list = &channel::setup().circle[root];
            this->self = (size + detail::self() - root) % size;
            this->dispatch();
        }
        return this->impl();
    }

} } }

#endif
#else
#define BIND_CHANNEL_NAME nop

#ifndef BIND_TRANSPORT_NOP
#define BIND_TRANSPORT_NOP

namespace bind { namespace transport { namespace nop {

    template<class T> struct collective {
        bool test(){ return true; }
        void append(rank_t rank){}
        bool involved(){ return true; }
    };

    class channel {
    public:
        typedef typename model::revision block_type;
        typedef typename model::any scalar_type;
        template<class T> using collective_type = collective<T>;
        size_t dim() const { return 1; }
        static void barrier(){}
        collective<block_type>* get(block_type& r){ return NULL; }
        collective<block_type>* set(block_type& r){ return NULL; }
        collective<scalar_type>* bcast(scalar_type& v, rank_t root){ return NULL; }
        collective<scalar_type>* bcast(scalar_type& v){ return NULL; }
        static constexpr rank_t rank = 0;
    };

} } }

#endif
#endif
#ifdef CUDART_VERSION

#ifndef BIND_TRANSPORT_CUDA_CHANNEL
#define BIND_TRANSPORT_CUDA_CHANNEL

#define NSTREAMS 16

namespace bind { namespace transport { namespace cuda {

    inline cudaError_t checkCuda(cudaError_t result){
        if(result != cudaSuccess) throw std::runtime_error("Error: CUDA transport failure");
        return result;
    }

    class channel {
    public:
        struct mount {
            mount(){
                for(int k = 0; k < NSTREAMS; k++) checkCuda( cudaStreamCreate(&streams[k]) );
            }
           ~mount(){
                for(int k = 0; k < NSTREAMS; k++) checkCuda( cudaStreamDestroy(streams[k]) );
                cudaDeviceReset();
            }
            cudaStream_t streams[NSTREAMS];
        };
        static mount& setup(){ 
            static mount m; 
            return m; 
        }
        channel(){
	    channel::setup(); // making sure CUDA is initialised
        }
    };

} } }

#undef NSTREAMS
#endif
#endif
// }}}
// {{{ core package (requires :model :transport)

#ifndef BIND_CORE_COLLECTOR
#define BIND_CORE_COLLECTOR

namespace bind{ namespace core {

    using model::revision;
    using model::any;

    class collector {
    public:
        void squeeze(revision* r) const {
            if((!r->referenced() || model::remote(r)) && r->locked_once()) r->spec.free(r->data);
        }
        void push_back(revision* r){
            r->weaken();
            if(!r->referenced()){ // squeeze
                if(!r->locked()) r->spec.free(r->data); // artifacts or last one
                this->rs.push_back(r);
            }
        }
        void push_back(any* o){
            this->as.push_back(o);
        }
        void clear(){
            for(auto r : rs){ r->spec.free(r->data); delete r; }
            for(auto a : as) memory::cpu::standard::free(a);
            rs.clear();
            as.clear();
        }
    private:
        std::vector<revision*> rs;
        std::vector<any*> as;
    };

} }

#endif

#ifndef BIND_CORE_NODE_H
#define BIND_CORE_NODE_H

namespace bind {

    namespace core {
        class controller;
    }

    class node {
    protected:
        typedef core::controller controller_type;
        node(){}
    public:
       ~node();
        node(std::vector<rank_t>::const_iterator it);
        node(const rank_t r);
        bool remote() const;
        bool local()  const;
        bool common() const;
        rank_t which()  const;
        template <class... Args> void cpu(Args&& ... args) const;
        template <class... Args> void gpu(Args&& ... args) const;
        friend class core::controller;
    protected:
        rank_t rank;
        locality state;
        controller_type* controller;
    };

    struct node_each : public node {
        node_each(typename node::controller_type* c);
    };

}

#endif

#ifndef BIND_CORE_CONTROLLER_H
#define BIND_CORE_CONTROLLER_H

namespace bind { namespace core {

    using model::history;
    using model::revision;
    using model::any;
    using model::functor;

    class controller {
        controller(const controller&) = delete;
        controller& operator=(const controller&) = delete;
        controller(); 
    public:
        typedef transport::BIND_CHANNEL_NAME::channel channel_type;
       ~controller();
        void flush();
        void clear();
        bool queue (functor* f);
        bool update(revision& r);

        template<locality L, device D, typename T> void sync(T* o);
        template<typename T> void collect(T* o);
        template<locality L, device D> void sync(revision*& c, revision*& s);
        void collect(revision* c, revision*& s);
        void squeeze(revision* r) const;

        template<device D>
        void touch(const history* o, rank_t owner);
        void use_revision(history* o);
        template<locality L, device D>
        void add_revision(history* o, functor* g, rank_t owner);

        bool verbose() const;
        rank_t get_rank() const;
        int get_num_procs() const;
        channel_type& get_channel();

        void sync();

        controller* activate(node* a);
        void deactivate(node* a);
        node& get_node();
    private:
        size_t clock;
        channel_type channel;
        std::vector< functor* > stack_m;
        std::vector< functor* > stack_s;
        std::vector< functor* >* chains;
        std::vector< functor* >* mirror;
        collector garbage;
        utils::funneled_io io_guard;
        node_each* each;
        node* which;
    public:
        std::vector<rank_t> nodes;
        template<class T>
        struct weak_instance {
            static controller w;
        };
    };
    
} }

namespace bind {
    template<class T> core::controller core::controller::weak_instance<T>::w;
    inline core::controller& select(){ return core::controller::weak_instance<void>::w; }
}

namespace bind { namespace nodes {
    inline size_t size(){
        return select().nodes.size();
    }
    inline std::vector<rank_t>::const_iterator begin(){
        return select().nodes.begin();
    }
    inline std::vector<rank_t>::const_iterator end(){
        return select().nodes.end();
    }
    inline rank_t which_(){
        return select().get_node().which();
    }
    template<typename V>
    inline rank_t which(const V& o){
        return o.allocator_.desc->current->owner;
    }
    inline rank_t which(){
        rank_t w = which_();
        return (w == select().get_num_procs() ? select().get_rank() : w);
    }
} }

#endif
#ifdef CUDART_VERSION

#ifndef BIND_TRANSPORT_CUDA_TRANSFER_H
#define BIND_TRANSPORT_CUDA_TRANSFER_H

namespace bind { namespace transport { namespace cuda {
    using model::functor;
    using model::revision;

    template<device From, device To>
    struct transfer : public functor, public memory::cpu::use_bulk_new<transfer<From, To> > {
	static void spawn(revision& r, revision& s);
	transfer(revision& r, revision& s);
	virtual void invoke() override;
	virtual bool ready() override;
    private:
	revision& r;
	revision& s;
    };

} } }

#endif

#ifndef BIND_TRANSPORT_CUDA_TRANSFER_HPP
#define BIND_TRANSPORT_CUDA_TRANSFER_HPP

namespace bind { namespace transport { namespace cuda {

    namespace detail {
        template<device From, device To>
        struct transfer_impl {};

        template<>
        struct transfer_impl<device::cpu, device::gpu> {
            using memory_type = memory::gpu::standard;
            static void invoke(void* dst, void* src, size_t sz){
                cudaMemcpy(dst, src, sz, cudaMemcpyHostToDevice);
            }
        };

        template<>
        struct transfer_impl<device::gpu, device::cpu> {
            using memory_type = memory::cpu::standard;
            static void invoke(void* dst, void* src, size_t sz){
                cudaMemcpy(dst, src, sz, cudaMemcpyDeviceToHost);
            }
        };
    }

    template<device From, device To>
    void transfer<From, To>::spawn(revision& r, revision& s){
        s.generator = new transfer(r, s);
    }

    template<device From, device To>
    transfer<From, To>::transfer(revision& r, revision& s) : r(r), s(s) {
        r.use(); s.use();
        s.embed(s.spec.hard_malloc<typename detail::transfer_impl<From, To>::memory_type>());
        if(r.generator != NULL) (r.generator.load())->queue(this);
        else bind::select().queue(this);
    }

    template<device From, device To>
    void transfer<From, To>::invoke(){
        detail::transfer_impl<From, To>::invoke(s.data, r.data, r.spec.extent);

        bind::select().squeeze(&r); r.release();
        bind::select().squeeze(&s); s.release();
        s.complete();
    }

    template<device From, device To>
    bool transfer<From, To>::ready(){
        return (r.generator == NULL);
    }

} } }

#endif
#endif

#ifndef BIND_CORE_GET_H
#define BIND_CORE_GET_H

namespace bind { namespace core {
    
    template<class T> class get {};

    template<>
    class get<any> : public functor, public memory::cpu::use_bulk_new<get<any> > {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(any& v);
        get(any& v);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        collective<any>* handle;
        any& t;
    };

    template<>
    class get<revision> : public functor, public memory::cpu::use_bulk_new<get<revision> >  {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(revision& r);
        get(revision& r);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        void operator += (rank_t rank);
    private:
        collective<revision>* handle;
        revision& t;
    };

} }

#endif


#ifndef BIND_CORE_GET_HPP
#define BIND_CORE_GET_HPP

namespace bind { namespace core {

    // {{{ any

    inline void get<any>::spawn(any& t){
        bind::select().queue(new get(t));
    }
    inline get<any>::get(any& ptr) : t(ptr) {
        handle = bind::select().get_channel().bcast(t, bind::nodes::which_());
        t.generator = this;
    }
    inline bool get<any>::ready(){
        return handle->test();
    }
    inline void get<any>::invoke(){
        t.complete();
    }

    // }}}
    // {{{ revision

    inline void get<revision>::spawn(revision& r){
        get*& transfer = (get*&)r.assist.second;
        if(bind::select().update(r)) transfer = new get(r);
        *transfer += bind::nodes::which_();
    }
    inline get<revision>::get(revision& r) : t(r) {
        handle = bind::select().get_channel().get(t);
        t.invalidate();
    }
    inline void get<revision>::operator += (rank_t rank){
        handle->append(rank);
        if(handle->involved() && !t.valid()){
            t.use();
            t.generator = this;
            t.embed(t.spec.hard_malloc<memory::cpu::standard>()); 
            bind::select().queue(this);
        }
    }
    inline bool get<revision>::ready(){
        return handle->test();
    }
    inline void get<revision>::invoke(){
        bind::select().squeeze(&t);
        t.release();
        t.complete();
    }

    // }}}

} }

#endif

#ifndef BIND_CORE_SET_H
#define BIND_CORE_SET_H

namespace bind { namespace core {

    template<class T> class set {};

    template<>
    class set<any> : public functor, public memory::cpu::use_bulk_new<set<any> > {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(any& v);
        set(any& v);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        collective<any>* handle;
        any& t;
    };

    template<>
    class set<revision> : public functor, public memory::cpu::use_bulk_new<set<revision> > {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(revision& r);
        set(revision& r);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        void operator += (rank_t rank);
    private:
        collective<revision>* handle;
        revision& t;
    };

} }

#endif

#ifndef BIND_CORE_SET_HPP
#define BIND_CORE_SET_HPP

namespace bind { namespace core {

    // {{{ any

    inline void set<any>::spawn(any& t){
        (t.generator.load())->queue(new set(t));
    }
    inline set<any>::set(any& t) : t(t) {
        handle = bind::select().get_channel().bcast(t, bind::nodes::which_());
    }
    inline bool set<any>::ready(){
        return (t.generator != NULL ? false : handle->test());
    }
    inline void set<any>::invoke(){}

    // }}}
    // {{{ revision

    inline void set<revision>::spawn(revision& r){
        set*& transfer = (set*&)r.assist.second;
        if(bind::select().update(r)) transfer = new set(r);
        *transfer += bind::nodes::which_();
    }
    inline set<revision>::set(revision& r) : t(r) {
        t.use();
        handle = bind::select().get_channel().set(t);
        if(t.generator != NULL) (t.generator.load())->queue(this);
        else bind::select().queue(this);
    }
    inline void set<revision>::operator += (rank_t rank){
        handle->append(rank);
    }
    inline bool set<revision>::ready(){
        return (t.generator != NULL ? false : handle->test());
    }
    inline void set<revision>::invoke(){
        bind::select().squeeze(&t);
        t.release(); 
    }

    // }}}

} }

#endif

#ifndef BIND_CORE_HUB
#define BIND_CORE_HUB

namespace bind { namespace transport {
    using model::revision;
    using model::any;

    template<typename T, device D, locality L> struct hub;
    template<locality L> class hub_mpi;

    // {{{ hub for model::any
    template<device D>
    struct hub<any, D, locality::local> {
        static void sync(any* v){
            if(bind::nodes::size() == 1) return;
            core::set<any>::spawn(*v);
        }
    };
    template<device D>
    struct hub<any, D, locality::remote> {
        static void sync(any* v){
            core::get<any>::spawn(*v);
        }
    };
    // }}}
    // {{{ mpi-only hub
    template<>
    struct hub_mpi<locality::common> {
        static void sync(revision* r){
            if(bind::nodes::size() == 1) return; // serial
            if(model::common(r)) return;
            if(model::local(r)) core::set<revision>::spawn(*r);
            else core::get<revision>::spawn(*r);
        }
    };
    template<>
    struct hub_mpi<locality::local> {
        static void sync(revision* r){
            if(model::remote(r)) core::get<revision>::spawn(*r);
        }
    };
    template<>
    struct hub_mpi<locality::remote> {
        static void sync(revision* r){
            if(r->owner == bind::nodes::which_() || model::common(r)) return;
            if(model::local(r)) core::set<revision>::spawn(*r);
            else core::get<revision>::spawn(*r); // assist
        }
    };
    // }}}
    #ifndef CUDART_VERSION
    // {{{ default hub for model::revision
    template<device D, locality L>
    struct hub<revision, D, L> {
        static void sync(revision* r, revision*){
            hub_mpi<L>::sync(r);
        }
    };
    // }}}
    #else
    // {{{ device-aware hub for model::revision
    // {{{ cpu
    template<locality L>
    struct hub<revision, device::cpu, L> {
        static void sync(revision*& r, revision*& s){
            if(model::gpu(r)){
                if(!s){
                    s = r->clone<device::cpu>(); if(!model::remote(r))
                    cuda::transfer<device::gpu, device::cpu>::spawn(*r, *s);
                }
                std::swap(r, s);
            }
            hub_mpi<L>::sync(r);
        }
    };
    template<>
    struct hub<revision, device::cpu, locality::remote> {
        static void sync(revision*& r, revision*& s){
            if(model::gpu(r)){
                if(!s){
                    s = r->clone<device::cpu>(); if(model::local(r))
                    cuda::transfer<device::gpu, device::cpu>::spawn(*r, *s);
                }
                std::swap(r, s);
            }
            hub_mpi<locality::remote>::sync(r);
        }
    };
    // }}}
    // {{{ gpu
    template<>
    struct hub<revision, device::gpu, locality::common> {
        static void sync(revision*& r, revision*& s){
            if(model::cpu(r)){
                hub_mpi<locality::common>::sync(r);
                if(!s){
                    s = r->clone<device::gpu>();
                    cuda::transfer<device::cpu, device::gpu>::spawn(*r, *s);
                }
                std::swap(r, s);
            }else{
                if(model::common(r)) return;
                if(!s){
                    s = r->clone<device::cpu>();        if(model::local(r))  cuda::transfer<device::gpu, device::cpu>::spawn(*r, *s);
                    hub_mpi<locality::common>::sync(s); if(model::remote(r)) cuda::transfer<device::cpu, device::gpu>::spawn(*s, *r);
                }else
                    hub_mpi<locality::common>::sync(s); // legacy refresh
            }
        }
    };
    template<>
    struct hub<revision, device::gpu, locality::local> {
        static void sync(revision*& r, revision*& s){
            if(model::cpu(r)){
                hub_mpi<locality::local>::sync(r);
                if(!s){
                    s = r->clone<device::gpu>();
                    cuda::transfer<device::cpu, device::gpu>::spawn(*r, *s);
                }
                std::swap(r, s);
            }else{
                if(!model::remote(r)) return;
                if(!s){
                    s = r->clone<device::cpu>(); hub_mpi<locality::local>::sync(s);
                    cuda::transfer<device::cpu, device::gpu>::spawn(*s, *r);
                }else
                    hub_mpi<locality::local>::sync(s); // legacy refresh
            }
        }
    };
    template<>
    struct hub<revision, device::gpu, locality::remote> {
        static void sync(revision*& r, revision*& s){
            hub<revision, device::cpu, locality::remote>::sync(r, s);
        }
    };
    // }}}
    // }}}
    #endif
} }

#endif

#ifndef BIND_CORE_CONTROLLER_HPP
#define BIND_CORE_CONTROLLER_HPP

namespace bind { namespace core {

    inline controller::~controller(){
        if(!chains->empty()) printf("Bind:: exiting with operations still in queue!\n");
        this->clear();
        delete this->each;
    }

    inline controller::controller() : chains(&stack_m), mirror(&stack_s), clock(1) {
        this->each = new node_each(this);
        this->which = NULL;
        for(int i = 0; i < get_num_procs(); i++) nodes.push_back(i);
        if(!verbose()) this->io_guard.enable();
    }

    inline void controller::deactivate(node* a){
        which = NULL;
    }

    inline controller* controller::activate(node* n){
        if(which) return NULL;
        which = n;
        return this;
    }

    inline void controller::sync(){
        this->flush();
        this->clear();
    }

    inline node& controller::get_node(){
        return (!which) ? *each : *which;
    }

    inline void controller::flush(){
        BIND_SMP_ENABLE
        while(!chains->empty()){
            for(auto task : *chains){
                if(task->ready()){
                    BIND_THREAD task->invoke();
                    for(auto d : task->deps) d->ready();
                    mirror->insert(mirror->end(), task->deps.begin(), task->deps.end());
                }else mirror->push_back(task);
            }
            chains->clear();
            std::swap(chains,mirror);
        }
        BIND_SMP_DISABLE
        clock++;
        channel.barrier();
    }

    inline void controller::clear(){
        this->garbage.clear();
        memory::cpu::instr_bulk::drop();
    }

    inline bool controller::queue(functor* f){
        this->chains->push_back(f);
        return true;
    }

    inline bool controller::update(revision& r){
        if(r.assist.first != clock){
            r.assist.first = clock;
            return true;
        }
        return false;
    }

    template<locality L, device D, typename T>
    inline void controller::sync(T* o){
        transport::hub<T, D, L>::sync(o);
    }

    template<typename T> void controller::collect(T* o){
        this->garbage.push_back(o);
    }

    template<locality L, device D>
    inline void controller::sync(revision*& c, revision*& s){
        transport::hub<revision, D, L>::sync(c, s);
    }

    void controller::collect(revision* c, revision*& s){
        collect(c); if(s){ collect(s); s = NULL; }
    }

    inline void controller::squeeze(revision* r) const {
        this->garbage.squeeze(r);
    }

    template<device D>
    inline void controller::touch(const history* o, rank_t owner){
        if(o->back() == NULL)
            const_cast<history*>(o)->init_state<D>(owner);
    }

    inline void controller::use_revision(history* o){
        o->back()->use();
    }

    template<locality L, device D>
    void controller::add_revision(history* o, functor* g, rank_t owner){
        o->add_state<L, D>(g, owner);
    }

    inline rank_t controller::get_rank() const {
        return channel.rank;
    }

    inline bool controller::verbose() const {
        return (get_rank() == 0);
    }

    inline int controller::get_num_procs() const {
        return channel.dim();
    }

    inline typename controller::channel_type & controller::get_channel(){
        return channel;
    }

} }

#endif

#ifndef BIND_CORE_NODE_HPP
#define BIND_CORE_NODE_HPP

namespace bind {

    // {{{ lambda interface shortcut

    template <class L, class... Args> void cpu(L l, Args&& ... args);
    template <class L, class... Args> void gpu(L l, Args&& ... args);

    template<class... Args>
    void node::cpu(Args&& ... args) const {
        bind::cpu(std::forward<Args>(args)...);
    }
    template<class... Args>
    void node::gpu(Args&& ... args) const {
        bind::gpu(std::forward<Args>(args)...);
    }

    // }}}
    // {{{ primary node-class

    inline node::~node(){
        if(!controller) return;
        bind::select().deactivate(this);
    }
    inline node::node(const rank_t r){
        if(! (controller = bind::select().activate(this)) ) return;
        this->state = (r == controller->get_rank()) ? locality::local : locality::remote;
        this->rank = r;
    }
    inline node::node(std::vector<rank_t>::const_iterator it) : node(*it)
    {
    }
    inline bool node::remote() const {
        return (state == locality::remote);
    }
    inline bool node::local() const {
        return (state == locality::local);
    }
    inline bool node::common() const {
        return (state == locality::common);
    }
    inline rank_t node::which() const {
        return this->rank;
    }

    // }}}
    // {{{ node's special case: everyone does the same

    inline node_each::node_each(typename node::controller_type* c){
        this->controller = c;
        this->rank = controller->get_num_procs();
        this->state = locality::common;
    }

    // }}}
}

#endif
// }}}
// {{{ interface package (requires :model :transport :core)

#ifndef BIND_INTERFACE_SHORTCUTS
#define BIND_INTERFACE_SHORTCUTS

namespace bind {

    inline void sync(){
        bind::select().sync();
    }

    inline int num_procs(){
        return bind::select().get_num_procs();
    }

    inline int num_threads(){
        static int n = BIND_NUM_THREADS; return n;
    }

    inline rank_t rank(){
        return bind::select().get_rank();
    }

    template<typename T>
    inline void collect(T* o){
        bind::select().collect(o);
    }

    inline void collect(model::revision* r, model::revision*& s){
        bind::select().collect(r, s);
    }

    template<typename V>
    inline bool weak(const V& obj){
        return obj.allocator_.desc->weak();
    }

    template<typename V>
    inline size_t extent(V& obj){
        return obj.allocator_.desc->extent;
    }

}

#endif

#ifndef BIND_INTERFACE_MODIFIERS_SINGULAR
#define BIND_INTERFACE_MODIFIERS_SINGULAR

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;

    template <typename T, bool Compact = false>
    struct singular_modifier {
        template<size_t Arg> static void deallocate(functor* ){ }
        template<size_t Arg> static bool pin(functor* ){ return false; }
        template<size_t Arg> static bool ready(functor* ){ return true; }
        template<size_t Arg> static void load(functor* m){ }
        template<size_t Arg> static T&   forward(functor* m){ EXTRACT(o); return o; }
        template<size_t Arg> static void apply_remote(T&){ }
        template<size_t Arg> static void apply_local(T& o, functor* m){
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T));
        }
        template<size_t Arg> static void apply_common(T& o, functor* m){
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T));
        }
        static constexpr bool ReferenceOnly = false;
    };

    template <typename T>
    struct singular_modifier<T, true> : public singular_modifier<T> {
        template<size_t Arg> static T& forward(functor* m){ return *(T*)&m->arguments[Arg]; }
        template<size_t Arg> static void apply_local(T& o, functor* m){ *(T*)&m->arguments[Arg] = o; }
        template<size_t Arg> static void apply_common(T& o, functor* m){ *(T*)&m->arguments[Arg] = o; }
    };
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_SHARED_PTR
#define BIND_INTERFACE_MODIFIERS_SHARED_PTR

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;

    template <typename T>
    struct const_shared_ptr_modifier : public singular_modifier<T> {
        template<size_t Arg> static bool ready(functor* m){
            EXTRACT(o);
            if(o.impl->origin && o.impl->origin->generator != NULL) return false;
            return (o.impl->generator == m || o.impl->generator == NULL);
        }
        template<size_t Arg> static bool pin(functor* m){
            EXTRACT(o);
            if(o.impl->generator == NULL) return false;
            (o.impl->generator.load())->queue(m);
            return true;
        }
        static constexpr bool ReferenceOnly = true;
    };

    template <typename T>
    struct shared_ptr_modifier : public const_shared_ptr_modifier<T> {
        template<size_t Arg> static void deallocate(functor* m){
            EXTRACT(o); o.impl->complete();
        }
        template<size_t Arg> static bool pin(functor* m){
            EXTRACT(o);
            if(!o.impl->origin || o.impl->origin->generator == NULL) return false;
            (o.impl->origin->generator.load())->queue(m);
            return true;
        }
        template<size_t Arg> static void apply_remote(T& o){
            o.resit();
            bind::select().sync<locality::remote, device::cpu>(o.impl);
        }
        template<size_t Arg> static void apply_local(T& o, functor* m){
            if(o.impl->generator != m){
                o.resit();
                o.impl->generator = m;
            }
            bind::select().sync<locality::local, device::cpu>(o.impl);
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T)); 
        }
        template<size_t Arg> static void apply_common(T& o, functor* m){
            if(o.impl->generator != m){
                o.resit();
                o.impl->generator = m;
            }
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T)); 
        }
        template<size_t Arg> static void load(functor* m){
            EXTRACT(o);
            if(o.impl->origin){
                *o.impl = (typename T::element_type&)*o.impl->origin;
                o.impl->origin = NULL;
            }
        }
    };

    template <typename T>
    struct volatile_shared_ptr_modifier : public shared_ptr_modifier<T> {
        template<size_t Arg> static void apply_remote(T& o){ }
        template<size_t Arg> static void apply_local(T& o, functor* m){
            apply_common<Arg>(o, m);
        }
        template<size_t Arg> static void apply_common(T& o, functor* m){
            shared_ptr_modifier<typename std::remove_volatile<T>::type>::apply_common<Arg>(const_cast<typename std::remove_volatile<T>::type&>(o), m);
        }
    };
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_ITERATOR
#define BIND_INTERFACE_MODIFIERS_ITERATOR

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;
    template <device D, typename T> struct modifier;

    template <device D, typename T>
    struct iterator_modifier : public singular_modifier<T> {
        typedef typename modifier<D, typename T::container_type>::type type;
        typedef typename T::container_type container_type;

        template<size_t Arg> 
        static void deallocate(functor* m){
            EXTRACT(o); type::deallocate_(*o.container);
        }
        template<size_t Arg>
        static void apply_remote(T& o){
            type::template apply_remote<Arg>(*o.container);
        }
        template<size_t Arg>
        static void apply_local(T& o, functor* m){
            type::template apply_local<Arg>(*o.container, m);
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &o, sizeof(T));
            var->container = (container_type*)m->arguments[Arg]; m->arguments[Arg] = (void*)var;
        }
        template<size_t Arg>
        static void apply_common(T& o, functor* m){
            type::template apply_common<Arg>(*o.container, m);
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &o, sizeof(T));
            var->container = (container_type*)m->arguments[Arg]; m->arguments[Arg] = (void*)var;
        }
        template<size_t Arg>
        static void load(functor* m){
            EXTRACT(o); type::load_(*o.container);
        }
        template<size_t Arg> 
        static bool pin(functor* m){ 
            EXTRACT(o); return type::pin_(*o.container, m);
        }
        template<size_t Arg> 
        static bool ready(functor* m){
            EXTRACT(o); return type::ready_(*o.container, m);
        }
    };
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_VERSIONED
#define BIND_INTERFACE_MODIFIERS_VERSIONED

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;
    using model::revision;

    template <device D, typename T>
    struct versioned_modifier : public singular_modifier<T> {
        template<size_t Arg> 
        static void deallocate(functor* m){
            EXTRACT(o); deallocate_(o);
        }
        template<size_t Arg> 
        static bool pin(functor* m){
            EXTRACT(o); return pin_(o,m);
        }
        template<size_t Arg> 
        static bool ready(functor* m){
            EXTRACT(o); return ready_(o, m);
        }
        template<size_t Arg>
        static void load(functor* m){ 
            EXTRACT(o); load_(o);
        }
        static void deallocate_(T& o){
            revision& parent  = *o.allocator_.before;
            revision& current = *o.allocator_.after;
            current.complete();
            current.release();
            bind::select().squeeze(&parent);
            parent.release();
        }
        template<locality L, size_t Arg>
        static void apply_(T& obj, functor* m){
            auto o = obj.allocator_.desc;
            bind::select().touch<D>(o, bind::rank());
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &obj, sizeof(T)); 
            m->arguments[Arg] = (void*)var;
            if(o->current->generator != m)
                bind::select().sync<L,D>(o->current, o->shadow);
            bind::select().use_revision(o);

            var->allocator_.before = o->current;
            if(o->current->generator != m){
                bind::select().collect(o->current, o->shadow);
                bind::select().add_revision<L, D>(o, m, bind::rank());
            }
            bind::select().use_revision(o);
            var->allocator_.after = obj.allocator_.after = o->current;
        }
        template<size_t Arg>
        static void apply_remote(T& obj){
            auto o = obj.allocator_.desc;
            bind::select().touch<D>(o, bind::rank());
            bind::select().sync<locality::remote, D>(o->current, o->shadow);
            bind::select().collect(o->current, o->shadow);
            bind::select().add_revision<locality::remote, D>(o, NULL, bind::nodes::which_()); 
        }
        template<size_t Arg>
        static void apply_local(T& obj, functor* m){
            apply_<locality::local, Arg>(obj, m);
        }
        template<size_t Arg>
        static void apply_common(T& obj, functor* m){
            apply_<locality::common, Arg>(obj, m);
        }
        static bool pin_(T& o, functor* m){
            revision& r = *o.allocator_.before;
            if(r.generator != NULL && r.generator != m){
                (r.generator.load())->queue(m);
                return true;
            }
            return false;
        }
        static bool ready_(T& o, functor* m){
            revision& r = *o.allocator_.before;
            if(r.generator == NULL || r.generator == m) return true;
            return false;
        }
        static void load_(T& o){ 
            revision& c = *o.allocator_.after; if(c.valid()) return;
            revision& p = *o.allocator_.before;
            if(!p.valid()){
                c.embed(c.spec.calloc<typename associated_memory_types<D>::set>());
            }else if(!p.locked_once() || p.referenced() || !c.spec.conserves<typename associated_memory_types<D>::set>(p.spec)){
                c.embed(c.spec.malloc<typename associated_memory_types<D>::set>());
                c.spec.memcpy<typename associated_memory_types<D>::set>(c.data, p.data, p.spec);
            }else
                c.reuse(p);
        }
        static constexpr bool ReferenceOnly = true;
    };
    // {{{ compile-time type modifier: const/volatile cases of the versioned types
    template <device D, typename T>
    struct const_versioned_modifier : public versioned_modifier<D, T> {
        template<size_t Arg>
        static void deallocate(functor* m){
            EXTRACT(o); deallocate_(o);
        }
        static void deallocate_(T& o){
            revision& r = *o.allocator_.before;
            bind::select().squeeze(&r);
            r.release();
        }
        template<size_t Arg>
        static void load(functor* m){ 
            EXTRACT(o); load_(o);
        }
        static void load_(T& o){
            revision& c = *o.allocator_.before;
            if(!c.valid()) c.embed(c.spec.calloc<typename associated_memory_types<D>::set>());
        }
        template<locality L, size_t Arg> static void apply_(T& obj, functor* m){
            auto o = obj.allocator_.desc;
            bind::select().touch<D>(o, bind::rank());
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &obj, sizeof(T)); m->arguments[Arg] = (void*)var;
            if(o->current->generator != m)
                bind::select().sync<L,D>(o->current, o->shadow);
            bind::select().use_revision(o);
            var->allocator_.before = var->allocator_.after = o->current;
        }
        template<size_t Arg> static void apply_remote(T& obj){
            auto o = obj.allocator_.desc;
            bind::select().touch<D>(o, bind::rank());
            bind::select().sync<locality::remote,D>(o->current, o->shadow);
        }
        template<size_t Arg> static void apply_local(T& obj, functor* m){
            apply_<locality::local, Arg>(obj, m);
        }
        template<size_t Arg> static void apply_common(T& obj, functor* m){
            apply_<locality::common, Arg>(obj, m);
        }
    };
    template <device D, typename T>
    struct volatile_versioned_modifier : public versioned_modifier<D, T> {
        template<size_t Arg> static void load(functor* m){ 
            EXTRACT(o); load_(o);
        }
        static void load_(T& o){
            revision& c = *o.allocator_.after; if(c.valid()) return; // can it occur?
            revision& p = *o.allocator_.before;
            if(!p.valid() || !p.locked_once() || p.referenced() || !c.spec.conserves<typename associated_memory_types<D>::set>(p.spec)){
                c.embed(c.spec.malloc<typename associated_memory_types<D>::set>());
            }else
                c.reuse(p);
        }
        template<locality L, size_t Arg> static void apply_(T& obj, functor* m){
            auto o = obj.allocator_.desc;
            bind::select().touch<D>(o, bind::rank());
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, (void*)&obj, sizeof(T)); m->arguments[Arg] = (void*)var;
            bind::select().use_revision(o);

            var->allocator_.before = o->current;
            if(o->current->generator != m){
                bind::select().collect(o->current, o->shadow);
                bind::select().add_revision<L, D>(o, m, bind::rank()); 
            }
            bind::select().use_revision(o);
            var->allocator_.after = obj.allocator_.after = o->current;
        }
        template<size_t Arg> static void apply_remote(T& obj){
            auto o = obj.allocator_.desc;
            bind::select().touch<D>(o, bind::rank());
            bind::select().collect(o->current, o->shadow);
            bind::select().add_revision<locality::remote, D>(o, NULL, bind::nodes::which_()); 
        }
        template<size_t Arg> static void apply_local(T& obj, functor* m){
            apply_<locality::local, Arg>(obj, m);
        }
        template<size_t Arg> static void apply_common(T& obj, functor* m){
            apply_<locality::common, Arg>(obj, m);
        }
        template<size_t Arg> static bool pin(functor* m){ return false; }
        template<size_t Arg> static bool ready(functor* m){ return true; }
        static bool pin_(T&, functor*){ return false; }
        static bool ready_(T&, functor*){ return true; }
    };
    // }}}
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_DISPATCH
#define BIND_INTERFACE_MODIFIERS_DISPATCH

namespace bind {
    namespace detail {
        template<typename T>
        constexpr bool compact(){ return sizeof(T) <= sizeof(void*); }

        template <typename T> struct has_versioning {
            template <typename T1> static typename T1::allocator_type::bind_type test(int);
            template <typename> static void test(...);
            enum { value = !std::is_void<decltype(test<T>(0))>::value };
        };
        template <bool Versioned, device D, typename T> struct get_modifier { typedef singular_modifier<T, compact<T>()> type; };
        template<device D, typename T> struct get_modifier<true, D, T> { typedef versioned_modifier<D, T> type; };

        template <bool Versioned, device D, typename T> struct const_get_modifier { typedef singular_modifier<const T, compact<T>()> type; };
        template<device D, typename T> struct const_get_modifier<true, D, T> { typedef const_versioned_modifier<D, const T> type; };

        template <bool Versioned, device D, typename T> struct volatile_get_modifier { typedef singular_modifier<volatile T, compact<T>()> type; };
        template<device D, typename T> struct volatile_get_modifier<true, D, T> { typedef volatile_versioned_modifier<D, volatile T> type; };
    }

    template <typename T> class proxy_iterator;
    template <typename T> class shared_ptr;

    template <device D, typename T> struct modifier {
        typedef typename detail::get_modifier<detail::has_versioning<T>::value, D, T>::type type;
    };
    template <device D, typename T> struct modifier<D, const T> {
        typedef typename detail::const_get_modifier<detail::has_versioning<T>::value, D, T>::type type;
    };
    template <device D, typename T> struct modifier<D, volatile T> {
        typedef typename detail::volatile_get_modifier<detail::has_versioning<T>::value, D, T>::type type;
    };
    template <device D, typename S> struct modifier<D, shared_ptr<S> > {
        typedef shared_ptr_modifier< shared_ptr<S> > type; 
    };
    template <device D, typename S> struct modifier<D, const shared_ptr<S> > {
        typedef const_shared_ptr_modifier< const shared_ptr<S> > type; 
    };
    template <device D, typename S> struct modifier<D, volatile shared_ptr<S> > {
        typedef volatile_shared_ptr_modifier< volatile shared_ptr<S> > type; 
    };
    template <device D, typename S> struct modifier<D, proxy_iterator<S> > {
        typedef iterator_modifier<D, proxy_iterator<S> > type;
    };
}

#endif

#ifndef BIND_INTERFACE_SNAPSHOT
#define BIND_INTERFACE_SNAPSHOT

namespace bind {

    using model::revision;

    struct snapshot {
        typedef model::history bind_type;

        snapshot& operator=(const snapshot&) = delete;
        snapshot(){ } // should be deleted
        snapshot(size_t size){
            desc = new bind_type(size);
        }
        snapshot(const snapshot& origin){
            desc = new bind_type(origin.desc->extent);
            revision* r = origin.desc->back(); if(!r) return;
            desc->current = r;
            r->protect();
        }
       ~snapshot(){
            if(!desc->weak()) bind::collect(desc->current, desc->shadow);
            delete desc;
        }
        void* data() volatile {
            return after->data;
        }
        revision* before = NULL;
        revision* after = NULL;
        bind_type* desc;
    };
}

#endif

#ifndef BIND_INTERFACE_KERNEL_INLINER
#define BIND_INTERFACE_KERNEL_INLINER

namespace bind {
    using model::functor;

    template<device D, typename T>
    struct check_if_not_reference {
        template<bool C, typename F>  struct fail_if_true { typedef F type; };
        template<typename F> struct fail_if_true<true, F> { };
        typedef typename fail_if_true<modifier<D, T>::type::ReferenceOnly, T>::type type; // T can be passed only by reference
    };

    template<device D, typename T>
    struct check_if_not_reference<D, T&> {
        typedef T type;
    };

    template <device D, typename T>
    using checked_remove_reference = typename std::remove_reference<
                                         typename check_if_not_reference< D, T >::type
                                     >::type;

    template<device D, int N> void expand_modify_remote(){}
    template<device D, int N> void expand_modify_local(functor* o){}
    template<device D, int N> void expand_modify_common(functor* o){}
    template<device D, int N> bool expand_pin(functor* o){ return false; }
    template<device D, int N> void expand_load(functor* o){ }
    template<device D, int N> void expand_deallocate(functor* o){ }
    template<device D, int N> bool expand_ready(functor* o){ return true; }

    template<device D, int N, typename T, typename... TF>
    void expand_modify_remote(T& arg, TF&... other){
        modifier<D, T>::type::template apply_remote<N>(arg);
        expand_modify_remote<D, N+1>(other...);
    }
    template<device D, int N, typename T, typename... TF>
    void expand_modify_local(functor* o, T& arg, TF&... other){
        modifier<D, T>::type::template apply_local<N>(arg, o);
        expand_modify_local<D, N+1>(o, other...);
    }
    template<device D, int N, typename T, typename... TF>
    void expand_modify_common(functor* o, T& arg, TF&... other){
        modifier<D, T>::type::template apply_common<N>(arg, o);
        expand_modify_common<D, N+1>(o, other...);
    }
    template<device D, int N, typename T, typename... TF>
    bool expand_pin(functor* o){
        return modifier<D, checked_remove_reference<D, T> >::type::template pin<N>(o) ||
               expand_pin<D, N+1, TF...>(o);
    }
    template<device D, int N, typename T, typename... TF>
    void expand_load(functor* o){
        modifier<D, checked_remove_reference<D, T> >::type::template load<N>(o);
        expand_load<D, N+1, TF...>(o);
    }
    template<device D, int N, typename T, typename... TF>
    void expand_deallocate(functor* o){
        modifier<D, checked_remove_reference<D, T> >::type::template deallocate<N>(o);
        expand_deallocate<D, N+1, TF...>(o);
    }
    template<device D, int N, typename T, typename... TF>
    bool expand_ready(functor* o){
        return modifier<D, checked_remove_reference<D, T> >::type::template ready<N>(o) &&
               expand_ready<D, N+1, TF...>(o);
    }

    template<device D, typename FP, FP fp>
    struct kernel_inliner {};

    template<device D, typename... TF , void(*fp)( TF... )>
    struct kernel_inliner<D, void(*)( TF... ), fp> {
        static const int arity = sizeof...(TF);

        static inline void latch(functor* o, TF&... args){
            #ifndef BIND_TRANSPORT_NOP
            if(bind::select().get_node().remote())   { expand_modify_remote<D, 0>(args...); return; }
            else if(bind::select().get_node().local()) expand_modify_local<D, 0>(o, args...);
            else
            #endif
            expand_modify_common<D, 0>(o, args...);
            expand_pin<D, 0, TF...>(o) || bind::select().queue(o);
        }
        static inline void cleanup(functor* o){
            expand_deallocate<D, 0, TF...>(o);
        }
        static inline bool ready(functor* o){
            return expand_ready<D, 0, TF...>(o);
        }
        template<size_t...I>
        static void expand_invoke(index_sequence<I...>, functor* o){
            (*fp)(modifier<D, checked_remove_reference<D, TF> >::type::template forward<I>(o)...);
        }
        static inline void invoke(functor* o){
            expand_load<D, 0, TF...>(o);
            expand_invoke(make_index_sequence<sizeof...(TF)>(), o);
        }
    };

}

#endif


#ifndef BIND_INTERFACE_KERNEL
#define BIND_INTERFACE_KERNEL

namespace bind {

    using model::functor;

    template<device D, class K>
    class kernel : public functor {
    public:
        #define inliner kernel_inliner<D, typename K::ftype, K::c>
        inline void operator delete (void* ptr){ }
        inline void* operator new (size_t size){
            return memory::cpu::instr_bulk::malloc<sizeof(K)+sizeof(void*)*inliner::arity>();
        }
        virtual bool ready() override { 
            return inliner::ready(this);
        }
        virtual void invoke() override {
            inliner::invoke(this);
            inliner::cleanup(this);
        }
        template<size_t...I, typename... Args>
        static void expand_spawn(index_sequence<I...>, Args&... args){
            inliner::latch(new kernel(), args...);
        }
        template<typename... Args>
        static inline void spawn(Args&& ... args){
            expand_spawn(make_index_sequence<sizeof...(Args)>(), args...);
        }
        #undef inliner
    };
}

#endif

#ifndef BIND_INTERFACE_LAMBDA
#define BIND_INTERFACE_LAMBDA

namespace bind {

    template<device D, bool Capture, typename F, typename... T>
    struct lambda_kernel : public kernel<D, lambda_kernel<D, Capture, F, T...> > {
        typedef void(*ftype)(T..., F*);
        static void fw(T... args, F* func){ (*func)(args...); delete func; }
        static constexpr ftype c = &fw;

        template<typename L, typename... Args>
        static void dispatch(L& func, Args&& ... values){
            kernel<D, lambda_kernel<D, Capture, F, T...> >::spawn(std::forward<Args>(values)..., new F(func));
        }
    };

    template<device D, typename F, typename... T>
    struct lambda_kernel<D, false, F, T...> : public kernel<D, lambda_kernel<D, false, F, T...> > {
        typedef void(*ftype)(T..., F&);
        static void fw(T... args, F& func){ func(args...); }
        static constexpr ftype c = &fw;

        template<typename L, typename... Args>
        static void dispatch(L& func, Args&& ... values){
            kernel<D, lambda_kernel<D, false, F, T...> >::spawn(std::forward<Args>(values)..., static_cast<F>(func));
        }
    };

    template <typename Function>
    struct function_traits : public function_traits<decltype(&Function::operator())> {
        static constexpr bool capture = !std::is_assignable<
             typename function_traits<decltype(&Function::operator())>::signature*&,
             Function
        >::value;

        template<device D>
        using kernel_type = typename function_traits<decltype(&Function::operator())>::template kernel_type<D, capture>;

        template<device D, typename... Args>
        static void dispatch(Function& func, Args&& ... values){
            kernel_type<D>::template dispatch<Function>(func, std::forward<Args>(values)...);
        }
    };

    template <typename ClassType, typename ReturnType, typename... Args>
    struct function_traits<ReturnType(ClassType::*)(Args...) const> {
        using signature = ReturnType(Args...);
        using function = const std::function<signature>;
        template<device D, bool Capture>
        using kernel_type = lambda_kernel<D, Capture, function, Args... >;
    };

    template <device D, class L>
    struct decorator : L {
        decorator(L l) : L(l) {}
        template <typename... T>
        void operator()(T&& ... values){
            function_traits<L>::template dispatch<D>(*(L*)this, std::forward<T>(values)...);
        }
    };

    template <device D, class... L, class R>
    struct decorator<D, R(*)(L...)> {
        decorator( R(*fp)(L...) ) : fp_(fp) {}
        template <typename... T>
        void operator()(T&& ... values){
            lambda_kernel<D, false, decltype(fp_), L... >::template dispatch<decltype(fp_)>(fp_, std::forward<T>(values)...);
        }
        R(*fp_)(L...);
    };

    template <device D = device::cpu, class L>
    decorator<D, L> decorate(L l){
        return decorator<D, L>(l);
    }

    template <class L, class... Args>
    void cpu(L l, Args&& ... args){
        decorate<device::cpu>(l)(std::forward<Args>(args)...);
    }

    template <class L, class... Args>
    void gpu(L l, Args&& ... args){
        decorate<device::gpu>(l)(std::forward<Args>(args)...);
    }
}

#endif

// }}}
// {{{ container package (requires :*)

#ifndef BIND_CONTAINER_SMART_PTR
#define BIND_CONTAINER_SMART_PTR

namespace bind {

    using model::any;
    using model::sizeof_any;

    template <typename T>
    class smart_ptr {
    protected:
        typedef T element_type;
       ~smart_ptr(){
           if(impl) bind::collect(impl); 
        }
        smart_ptr(element_type val){
            impl = new (memory::cpu::standard::calloc<sizeof_any<T>()>()) any(val);
        }
        smart_ptr(const smart_ptr& f){
            impl = new (memory::cpu::standard::calloc<sizeof_any<T>()>()) any((element_type&)*f);
            impl->origin = f.impl;
        }
        smart_ptr(smart_ptr&& f){
            impl = f.impl; f.impl = NULL; 
        }
        smart_ptr& operator= (const smart_ptr& f){
            *impl = (element_type&)*f;
            impl->origin = f.impl;
            return *this;
        }
        smart_ptr& operator= (smart_ptr&& f){ 
            std::swap(impl, f.impl);
            return *this;
        }
        template<typename S>
        smart_ptr& operator= (const S& val) = delete;
    public:
        T& operator* () const volatile {
            return *impl;
        }
        void resit() const {
            smart_ptr clone(*this);
            std::swap(this->impl, clone.impl);
            this->impl->origin = clone.impl;
        }
        mutable any* impl;
    };

    template<typename T>
    class shared_ptr : public smart_ptr<T> {
    public:
        typedef typename smart_ptr<T>::element_type element_type;
        template <typename U>
        shared_ptr(U&& arg) : smart_ptr<T>(std::forward<U>(arg)) {}
    };

    template<class T>
    std::ostream& operator << (std::ostream& os, const smart_ptr<T>& obj){
        os << *obj;
        return os;
    }
}

#endif

#ifndef BIND_CONTAINER_PROXY_ITERATOR
#define BIND_CONTAINER_PROXY_ITERATOR

namespace bind {

    template<class Container>
    class proxy_iterator {
    public:
        typedef Container container_type;
        typedef typename std::iterator_traits<bind::proxy_iterator<Container> >::value_type value_type;

        proxy_iterator() : container(NULL), position(0) {}
        proxy_iterator(container_type& owner, size_t p) : container(&owner), position(p) {}
        proxy_iterator& operator += (size_t offset){
            position += offset;
            return *this;
        }
        proxy_iterator& operator++ (){
            position++;
            return *this;
        }
        proxy_iterator operator++ (int){
            proxy_iterator tmp(*this);
            operator++();
            return tmp;
        }
        proxy_iterator& operator -= (size_t offset){
            position -= offset;
            return *this;
        }
        proxy_iterator& operator-- (){
            position--;
            return *this;
        }
        proxy_iterator operator-- (int){
            proxy_iterator tmp(*this);
            operator--();
            return tmp;
        }
        value_type& operator* () const {
            return (*container)[position];
        }
        container_type& get_container(){
            return *container;
        }
        const container_type& get_container() const {
            return *container;
        }
        size_t position;
    public:
        template<typename T>
        friend bool operator == (const proxy_iterator<T>& lhs, const proxy_iterator<T>& rhs);
        template<typename T>
        friend bool operator != (const proxy_iterator<T>& lhs, const proxy_iterator<T>& rhs);
        container_type* container;
    };

    template <class Container> 
    bool operator == (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position == rhs.position && lhs.container->allocator_.desc == rhs.container->allocator_.desc);
    }

    template <class Container> 
    bool operator != (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position != rhs.position || lhs.container->allocator_.desc != rhs.container->allocator_.desc);
    }

    template <class Container, class OtherContainer> 
    size_t operator - (const proxy_iterator<Container>& lhs, const proxy_iterator<OtherContainer>& rhs){ 
        return lhs.position - rhs.position;
    }

    template <class Container> 
    proxy_iterator<Container> operator + (proxy_iterator<Container> lhs, size_t offset){ 
        return (lhs += offset);
    }

    template <class Container> 
    proxy_iterator<Container> operator - (proxy_iterator<Container> lhs, size_t offset){ 
        return (lhs -= offset);
    }

    template <class Container> 
    bool operator < (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position < rhs.position);
    }

    template <class Container> 
    bool operator > (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position > rhs.position);
    }

}

namespace std {

    template<class Container>
    class iterator_traits<bind::proxy_iterator<Container> > {
    public:
        typedef std::random_access_iterator_tag iterator_category;
        typedef typename Container::value_type value_type;
        typedef size_t difference_type;
    };

    template<class Container>
    class iterator_traits<bind::proxy_iterator<const Container> > {
    public:
        typedef std::random_access_iterator_tag iterator_category;
        typedef const typename Container::value_type value_type;
        typedef size_t difference_type;
    };
}

#endif

#ifndef BIND_CONTAINER_ARRAY
#define BIND_CONTAINER_ARRAY

namespace bind {

    // {{{ array helper functions
    template<class T, class Allocator> class array;
    namespace detail {
        template<class T, class Allocator>
        void fill_array(volatile bind::array<T,Allocator>& a, T& value){
            for(size_t i = 0; i < a.size(); ++i) a[i] = value;
        }
        template<class T, class Allocator, class OtherAllocator = Allocator>
        void copy_array(volatile bind::array<T,Allocator>& dst, const bind::array<T,OtherAllocator>& src, const size_t& n){
            for(size_t i = 0; i < n; ++i) dst[i] = src[i];
        }
    }
    // }}}

    template <class T, class Allocator = bind::snapshot>
    class array {
    public:
        void* operator new (size_t size, void* ptr){ return ptr; }
        void  operator delete (void*, void*){ /* doesn't throw */ }
        void* operator new (size_t sz){ return memory::cpu::standard::malloc<sizeof(array)>(); }
        void operator delete (void* ptr){ memory::cpu::standard::free(ptr); }
    public:
        using allocator_type = Allocator;
        using value_type = T;
        using size_type = size_t;
        using volatile_iterator = proxy_iterator<volatile array>;
        using const_iterator = proxy_iterator<const array>;
        using iterator = proxy_iterator<array>;
        explicit array(){}
        array(const array& a) = default;
        explicit array(size_t n) : allocator_(n*sizeof(T)), size_(n) {}

        array& operator = (const array& rhs){
            array c(rhs);
            this->swap(c);
            return *this;
        }
        template<class OtherAllocator>
        array& operator = (const array<T,OtherAllocator>& rhs){
            array resized(rhs.size());
            this->swap(resized);
            if(!bind::weak(rhs)) bind::cpu(detail::copy_array<T,Allocator,OtherAllocator>, *this, rhs, this->size_);
            return *this;
        }
        void fill(T value){
            bind::cpu(detail::fill_array<T,Allocator>, *this, value);
        }
        void swap(array<T,Allocator>& r){
            std::swap(this->size_, r.size_);
            std::swap(this->allocator_.after->data, r.allocator_.after->data); // fixme
        }
        size_t size() const volatile {
            return size_;
        }
        bool empty() const volatile {
            return ((size() == 0) || bind::weak(*this));
        }
        value_type* data() volatile {
            return (value_type*)allocator_.data();
        }
        const value_type* data() const volatile {
            return (value_type*)allocator_.data();
        }
        value_type& operator[](size_t i) volatile {
            return data()[ i ];
        }
        const value_type& operator[](size_t i) const volatile {
            return data()[ i ];
        }
        value_type& at(size_type i) volatile {
            if(i >= size()) throw std::out_of_range("array::out_of_range");
            return (*this)[i];
        }
        const value_type& at(size_type i) const volatile {
            if(i >= size()) throw std::out_of_range("array::out_of_range");
            return (*this)[i];
        }
        value_type& front() volatile {
            return (*this)[0];
        }
        const value_type& front() const volatile {
            return (*this)[0];
        }
        value_type& back() volatile {
            return (*this)[size()-1];
        }
        const value_type& back() const volatile {
            return (*this)[size()-1];
        }
        iterator begin() volatile {
            return iterator(const_cast<array&>(*this), 0);
        }
        iterator end() volatile {
            return iterator(const_cast<array&>(*this), size());
        }
        const_iterator begin() const volatile {
            return cbegin();
        }
        const_iterator end() const volatile {
            return cend();
        }
        const_iterator cbegin() const volatile {
            return const_iterator(const_cast<const array&>(*this), 0);
        }
        const_iterator cend() const volatile {
            return const_iterator(const_cast<const array&>(*this), size());
        }
        volatile_iterator vbegin() volatile {
            return volatile_iterator(*this, 0);
        }
        volatile_iterator vend() volatile {
            return volatile_iterator(*this, size());
        }
    private:
        mutable size_t size_;
    public:
        mutable allocator_type allocator_;
    };

}

#endif

#ifndef BIND_CONTAINER_BLOCK
#define BIND_CONTAINER_BLOCK

namespace bind {
     
    template<typename T, class Allocator = bind::snapshot> class block;
    namespace detail { 
        template<typename T>
        void fill_value(volatile block<T>& a, T& value){
            size_t size = a.num_rows()*a.num_cols();
            T* ad = a.data();
            for(size_t i = 0; i < size; ++i) ad[i] = value;
        }
    }

    template <class T, class Allocator>
    class block {
    public:
        typedef Allocator allocator_type;
        typedef T value_type;
        block(size_t m, size_t n) : allocator_(sizeof(T)*m*n), rows(m), cols(n) {}
        void init(T value){
            bind::cpu(detail::fill_value<T>, *this, value);
        }
        value_type& operator()(size_t i, size_t j){
            return data()[ j*rows + i ];
        }
        const value_type& operator()(size_t i, size_t j) const {
            return data()[ j*rows + i ];
        }
        value_type* data() volatile {
            return (value_type*)allocator_.data();
        }
        const value_type* data() const volatile {
            return (value_type*)allocator_.data();
        }
        size_t num_rows() const volatile {
            return rows;
        }
        size_t num_cols() const volatile {
            return cols;
        }
        size_t rows;
        size_t cols;
        mutable allocator_type allocator_;
    };

}

#endif
// }}}
#ifdef BIND_NO_DEBUG
#undef NDEBUG
#endif
#endif
